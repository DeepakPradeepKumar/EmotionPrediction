# EmotionPrediction
Predictions of Human Emotions Using EEG Devices

Deepak Pradeep Kumar

ABSTRACT: 
This study is aimed to classify different emotions based on the expressions displayed / experienced by, thirty-five different participants of different age & gender, using the EEG device (Mindwave device). Participants voluntarily took part in this process and were asked to experience different emotions like, Happy, Sad, Angry, Stressed & Hungry. These experiences were deliberately experienced by these participants during the test., this helped us capture their frontal lobe’s brain functionality. Participants were specifically asked to experience certain emotions to observe the trend and capture the range of frequencies that they every emotion falls under based on their age and gender. EEG based functionality and data capture shows significantly different signals and frequency range during different emotional states. This data and pattern were used in the classification techniques for analysis to understand how emotions are experienced in different humans and to use this data for further predictions of certain emotions that are being emitted/experienced by the different humans. This study will provide a very useful tool to understand & predict emotions that are experienced by different humans at any given point of time.
PROBLEM STATEMENT:
We often experience human beings suffering from multiple emotional stress that could cause the human beings to not be in stable state of mind. It could be due to various reasons like the environment, circumstances and other human beings as well. Hence, we are trying to understand what kind of emotions that any human being is experiencing and could suggest them to seek help from appropriate resources.
1.	INTRODUCTION:
 
Emotion is a complex feature and predicting emotions using certain devices or Machine Learning is very complex process –and hence, examining the subjects using EEG devices to recognize different emotional states of the subjects and understand the pattern of the range of frequencies emitted by the device showing various emotions that can help predict emotions and use device to continuously extract the data to be fed to machines and use Machine learning techniques to continuously predict the emotions as accurately as possible.
There are millions of human beings right from just born to corporates to elderly people of whom we are unable to understand what kind of emotions they are going through. The study of predicting human emotions might help us to understand the other person’s emotions of what they are experiencing and how we can respond to the same – this might also help to keep a track of elderly people who are unable to express their emotions, whether they are hungry or sad or angry or even stressed. Understanding such emotions and addressing them can help humans lead a better life. Even with Corporate employees who are juggling with multiple things also go through stress and sadness. When they are unable to cope up with things around them, they tend to lose balance in life and might cause a worry to their near and dear ones, but, still unable to understand what they are going through. Hence, using the emotion-prediction system, we can understand what any person is going through, understand and lead them to find support. This can certainly help in every way of making humans get closer to each other for the betterment of life.
In this paper, we are trying to classify the human emotions based on the intensity, frequency, attention, and raw signals produced by the EEG (Electroencephalogram) devices when a human being/subject is experiencing certain emotions.
Subject is asked specifically to experience certain emotions e.g., (Happy, angry, sad, stressed, and hungry). Based on these emotions, frequencies, attention level and raw signals vary to a huge extent. The frequency level is little low in the kids but, the attention level is high – similarly, frequency is high and attention level is low in adults. There’s huge variation in frequencies and attention levels which produces a wide range for every emotion that is experienced by the subject. This data is used for analysis and classified based on the frequencies and attention levels captured by the device. More complex process is assessment of human emotions, the selection of measurement, results, and the evaluation methods. However, we are trying to just classify the emotions and predict them to understand what emotion a human being is going through that has been captured by the device.
Many studies have indicated that the physiological correlates of emotions that are likely to be found in the central nervous system (CNS) rather than simply in peripheral physiological responses [8–10]. Researchers have supported this viewpoint using electroencephalogram (EEG) & other neuro imaging devices to investigate specific activities of the brain associated with different emotional states. However, most of the available studies on emotion-specific EEG response have focused on EEG characteristics at the single-electrode level. Sophisticated devices used for neuroimaging that examine interrelated activity among multiple brain sites may hold more promise for understanding how the emotions are invoked and what other brain functionalities are involved in certain emotions that can be tracked with much accuracy.
Although, previous studies have tried to classify emotional states by means of capturing and analyzing EEG signals. We are trying to classify human emotions based on the frequencies, raw data, attention levels, alpha and beta signals
2.	EXPERMIENTS & PREDICTION SYSTEMS
The process of capturing the data from EEG devices was also exhausting., There were about 35 volunteers, varying from a range of 7 years to 50 years of both the genders, who wished to use the device for more than 15 mins and express various emotions during this time. Subjects were specifically asked to experience certain emotions that can help capture/observe the trend and behavior of frequencies, raw data, alpha, delta, and their attention levels at every emotion. 
This data was analyzed to understand the range of frequencies for various emotions like Happy, Angry, Sad, Stressed and Hungry.
The range of raw signal and frequency emitted by the device for Happiness was vast whereas Stressed and sad was showing a little with high intensity. Attention & alpha waves were also high whereas sad and angry shows fluctuations in the attention and low with frequencies. Positive emotions trigger the frequency and raw data levels to highest in adults but, the frequencies in kids do not cross a level of 18K with happier emotions.
Although, it is very hard to classify between sad and stressed emotions as they emit the almost the same range of frequencies., Attention and alpha waves differ to an extent. This helped us understand the classification between Emotions and predict them accordingly. 
3.	TRAINING & TEST RESULTS
Training the data went through the challenging phase and running through the feature scaling, signal normalization and sampling the data to identify right set of features and also scale the data to ensure that the right emotions are trained into the model. 
We were able to come up with the below table of data that showed different range of frequencies for various emotions. This also led us to continue with our research and train the model with multiple Machine Learning Algorithms.
 
 

Training the Data with Various Machine Learning Algorithms: 
We tried multiple algorithms including the neural network on a TensorFlow framework, it was amazing to see how the data was diverging after 45% of accuracy. Then again, tried back with classic ML algorithms and individually it was not going beyond 56% accuracy. We tried with multiple algorithms like XGBoost, RandomForest and K-Nearest Neighbors. However, all these algorithms individually, were providing much of accuracy, this made us change it to Ensemble technique. Merged these algorithms and ran the predictions to integrate the algorithms together and ran the predictions. This helped us reach close to 86% accuracy. Ran with multiple test sets and cross validation sets to make sure that the accuracy is close to the training set accuracy. 
Below shows one of the sample test set that we asked one of our colleagues to take a quick test and captured the data by using the EEG device on him. We were able to capture his emotions by having him wear the device for close to 8 mins. This data was captured right in front of him and displayed the emotional level that he is at.
 
This was also in line with what he was going through. We conducted this experiment during the hackathon session and have many people wear the device and help us capture the emotions. Most of the emotions predicted were appropriate to what subjects were going through. 
It is noteworthy that humans / subjects cannot always have one emotion at a time, and they will have multiple emotions going on in their mind. Hence, we are not predicting only one emotion of the subjects but, we are showing the percentage of human emotions that a subject is going through. Any emotion that is dominating can take the precedence. Given that, the EEG is a single electrode device and it is capturing only the frequencies and signals of the brainwave only from the frontal lobe of the brain, we were able to capture and predict the emotions and derive that a person/human is going through a particular emotion strongly. However, narrowing down to the one dominating emotion is beyond the scope of this study. However, that study needs multi electrode device that can capture frequencies from different parts of the brain and continue with appropriate prediction. This can also be taken further with associating the heart rate to make it better.
4.	CONCLUSION
This entire set up was a hackathon idea and that was conducted on some of the acquaintances of the author/publisher who wanted to check if predicting human emotions using one single electrode device attached to the brain is possible or not.  For further prediction of emotional state of the subject under laboratory conditions multiple equipment are needed that can help subject to trigger and have certain emotions felt and capture the signals and frequencies that will make the predictions even more accurate. The accuracy of predictions in this case has been close to 80% across multiple subjects and we were able to successfully predict human emotions of any random subjects with at least 80% accuracy. The pattern in which the signals are being generated/captured for different subjects vary a lot based on the gender, age, and the intensity in which they experience and think at certain levels.





5.	REFERENCES

1.	Classifying Different Emotional States by Means of EEGBased Functional Connectivity Patterns You-Yun Lee1 , Shulan Hsieh1,2*., https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0095415 
2.	Jérémie Nicolle, Vincent Rapp, Kévin Bailly, Lionel Prevost, Mohamed Chetouani Robust Continuous Prediction of Human Emotions using Multiscale Dynamic Cues
https://www.researchgate.net/publication/236629590_Robust_continuous_prediction_of_human_emotions_using_multiscale_dynamic_cues 
3.	Emotion Carrier Recognition from Personal Narratives, Aniruddha Tammewar, Alessandra Cervone, Giuseppe Riccardi 2020, https://arxiv.org/abs/2008.07481 
4.	V. Kollia, Personalization Effect on Emotion Recognition from Physiological Data: An Investigation of Performance on Different Setups and Classifiers, 2016, https://arxiv.org/abs/1607.05832.
5.	Unsupervised Learning in Reservoir Computing for EEG-based Emotion Recognition., Rahma Fourati ; Boudour Ammar ; Javier Sanchez-Medina ; Adel M. Alimi https://ieeexplore.ieee.org/abstract/document/9043472
6.	EEG-Based Emotion Recognition Using Regularized Graph Neural Networks. https://arxiv.org/pdf/1907.07835.pdf ., https://arxiv.org/abs/1907.07835
7.	Robust continuous prediction of human emotions using multiscale dynamic cues., https://www.researchgate.net/publication/236629590_Robust_continuous_prediction_of_human_emotions_using_multiscale_dynamic_cues
8.	Micro-Facial Expression Recognition in Video Based on Optimal Convolutional Neural Network (MFEOCNN) Algorithm
https://arxiv.org/abs/2009.13792
9.	Understanding Brain Dynamics for Color Perception using Wearable EEG headband., https://arxiv.org/abs/2008.07092
10.	End-to-End Multimodal Emotion Recognition using Deep Neural Networks., https://arxiv.org/abs/1704.08619
